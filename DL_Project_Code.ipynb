{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Deep Learning 23/24\n","### Project\n","\n","Beatriz Moreira, FC54514 \\\n","Rute Patuleia, FC51780 \\\n","Tiago Assis, FC62609"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-29T01:32:11.850648Z","iopub.status.busy":"2024-05-29T01:32:11.850278Z","iopub.status.idle":"2024-05-29T01:32:41.707561Z","shell.execute_reply":"2024-05-29T01:32:41.705948Z","shell.execute_reply.started":"2024-05-29T01:32:11.850618Z"},"trusted":true},"outputs":[],"source":["import warnings \n","warnings.filterwarnings('ignore')\n","import os\n","import shutil\n","import random\n","import time\n","from datetime import datetime\n","import gc \n","import pandas as pd\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data import DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard\n","import torchvision.transforms as FT\n","!pip install torchinfo\n","from torchinfo import summary\n","import nibabel as nib\n","import glob\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["## Data loading, preprocessing and augmenting"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Defining a class to handle data augmentations\n","class Augmentations:\n","    def __init__(self, img, mask):\n","        self.img = img\n","        self.mask = mask\n","       \n","    \n","    def rotate_scale(self):\n","        \"\"\"\n","        Rotates and scales images equally on every dimension. Scaling and rotation are applied with a probability of 0.2 each.\n","        The angles of rotation (in degrees) are each drawn from U(-30, 30).\n","        Scaling is implemented via multiplying coordinates with a scaling factor in the voxel grid.\n","        Thus, scale factors smaller than one result in a \"zoom out\" effect while values larger one result in a \"zoom in\" effect. \n","        The scaling factor is sampled from U(0.7, 1.4).\n","        Interpolation for images is done linearly, while masks are interpolated using the nearest pixels.\n","        \"\"\"\n","        angle = random.uniform(-30, 30)\n","        if angle < 0: # FT.functional.rotate only works with positive angles\n","            angle += 360\n","        scale_factor = random.uniform(0.7, 1.4)\n","        if random.random() < 0.2:\n","            self.mask = FT.functional.rotate(self.mask,angle, interpolation=FT.InterpolationMode.NEAREST)\n","            self.img = FT.functional.rotate(self.img, angle, interpolation=FT.InterpolationMode.BILINEAR)\n","        if random.random() < 0.2:\n","            self.mask = F.interpolate(self.mask.unsqueeze(0), \n","                                      [int(shape * scale_factor) for shape in self.mask.shape[1:]], \n","                                      mode=\"nearest\").squeeze(0)\n","            self.img = F.interpolate(self.img.unsqueeze(0), \n","                                     [int(shape * scale_factor) for shape in self.img.shape[1:]], \n","                                     mode=\"trilinear\", \n","                                     align_corners=True).squeeze(0)\n","        return self.img, self.mask\n","        \n","        \n","    def gaussian_noise(self):\n","        \"\"\"\n","        Zero centered additive Gaussian noise is added to each voxel in the sample independently. \n","        This augmentation is applied with a probability of 0.15. The variance of the noise is drawn from U(0, 0.1).\n","        \"\"\"\n","        if random.random() < 0.15:\n","            variance = random.uniform(0, 0.1)\n","            noise = torch.randn_like(self.img) * torch.sqrt(torch.tensor(variance))\n","            self.img += noise\n","        return self.img\n","        \n","        \n","    def gaussian_blur(self):\n","        \"\"\"\n","        Blurring is applied with a probability of 0.2 per sample. \n","        If this augmentation is triggered in a sample, blurring is applied with a probability of 0.5 for each of the associated modalities. \n","        The width (in voxels) of the Gaussian kernel, sigma, is sampled from U(0.5, 1.5) independently for each modality.\n","        \"\"\"\n","        if random.random() < 0.2:\n","            for channel in range(self.img.shape[0]):\n","                if random.random() < 0.5:\n","                    sigma = random.uniform(0.5, 1.5)\n","                    self.img[channel,:,:,:] = FT.functional.gaussian_blur(self.img[channel,:,:,:], kernel_size=3, sigma=sigma)\n","        return self.img\n","    \n","    \n","    def brightness(self):\n","        \"\"\"\n","        Voxel intensities are multiplied by x ~ U(0.7, 1.3) with a probability of 0.15.\n","        \"\"\"\n","        if random.random() < 0.15:\n","            brightness_factor = torch.tensor(np.random.uniform(0.7, 1.3))\n","            self.img *= brightness_factor\n","        return self.img\n","        \n","        \n","    def contrast(self):\n","        \"\"\"\n","        Voxel intensities are multiplied by x ~ U(0.65, 1.5) with a probability of 0.15.\n","        Following multiplication, the values are clipped to their original value range.\n","        \"\"\"\n","        if random.random() < 0.15:\n","            contrast_factor = torch.tensor(np.random.uniform(0.65, 1.5))\n","            min_val = torch.min(self.img)\n","            max_val = torch.max(self.img)\n","            self.img *= contrast_factor\n","            self.img = torch.clamp(self.img, min_val, max_val)\n","        return self.img\n","    \n","    \n","    def low_resolution(self):\n","        \"\"\"\n","        This augmentation is applied with a probability of 0.25 per sample and 0.5 per associated modality. \n","        Triggered modalities are downsampled by a factor of U(1, 2) using nearest neighbor interpolation \n","        and then sampled back up to their original size with cubic interpolation.\n","        \"\"\"\n","        if random.random() < 0.25:\n","            for channel in range(self.img.shape[0]):\n","                if random.random() < 0.5:\n","                    scale_factor = np.random.uniform(1, 2)\n","                    scaled = F.interpolate(self.img[channel,:,:,:].unsqueeze(0).unsqueeze(0), \n","                                           [int(shape * 1 / scale_factor) for shape in self.img.shape[1:]], \n","                                           mode='nearest')\n","                    scaled = F.interpolate(scaled, \n","                                           self.img.shape[1:], \n","                                           mode='trilinear', \n","                                           align_corners=True).squeeze(0).squeeze(0)\n","                    self.img[channel,:,:,:] = scaled\n","        return self.img\n","        \n","        \n","    def gamma(self):\n","        \"\"\"\n","        This augmentation is applied with a probability of 0.15. \n","        The patch intensities are scaled to a factor of [0, 1] of their respective value range. \n","        Then, a nonlinear intensity transformation is applied per voxel: { i_new = i_old ** gamma } with gamma ~ U(0.7, 1.5).\n","        The voxel intensities are subsequently scaled back to their original value range. \n","        With a probability of 0.15, this augmentation is applied with the voxel intensities being inverted\n","        prior to transformation: { (1 - i_new) = (1 - i_old) ** gamma }\n","        \"\"\"\n","        if random.random() < 0.15:\n","            gamma = torch.tensor(random.uniform(0.7, 1.5))\n","            min_val = torch.min(self.img)\n","            max_val = torch.max(self.img)\n","            self.img = (self.img - min_val) / (max_val - min_val)\n","            if random.random() < 0.15:\n","                self.img = 1 - self.img\n","            self.img = self.img ** gamma\n","            self.img = self.img * (max_val - min_val) + min_val\n","        return self.img\n","        \n","        \n","    def mirror(self):\n","        \"\"\"\n","        All patches are mirrored along all axes with a probability of 0.5.\n","        \"\"\"\n","        if random.random() < 0.5:\n","            axes = tuple(range(len(self.img.shape[1:])))\n","            self.img = torch.flip(self.img, dims=axes)\n","            self.mask = torch.flip(self.mask, dims=axes)\n","        return self.img, self.mask\n","    \n","    \n","    def transforms(self):\n","        self.img, self.mask = self.rotate_scale()\n","        self.img = self.gaussian_noise()\n","        self.img = self.gaussian_blur()\n","        self.img = self.brightness()\n","        self.img = self.contrast()\n","        self.img = self.low_resolution()\n","        self.img = self.gamma()\n","        self.img, self.mask = self.mirror()\n","        return self.img, self.mask"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.710424Z","iopub.status.busy":"2024-05-29T01:32:41.709485Z","iopub.status.idle":"2024-05-29T01:32:41.725655Z","shell.execute_reply":"2024-05-29T01:32:41.724731Z","shell.execute_reply.started":"2024-05-29T01:32:41.710394Z"},"trusted":true},"outputs":[],"source":["# Defining a class to handle image loading, preprocessing and augmentations\n","class BratsDataset(Dataset):\n","    def __init__(self, root_paths, test=False):\n","        self.test = test\n","        self.root_paths = root_paths\n","        # Getting all image paths from the 3 parts\n","        self.folder_paths = sorted([f\"{root_path}/{folder}\" for root_path in root_paths for folder in os.listdir(f\"{root_path}\")])\n","\n","\n","    def __getitem__(self, index):\n","        folder = self.folder_paths[index]\n","        combined_img = self.preprocess_images(folder)\n","        # If not test set, work with images and mask, otherwise only the images\n","        if not self.test:\n","            mask = self.preprocess_masks(folder)\n","            return combined_img, mask\n","        return combined_img, folder\n","\n","\n","    def preprocess_images(self, folder):\n","        imgs = []\n","        for modality in [\"-t1c.nii\", \"-t1n.nii\", \"-t2w.nii\", \"-t2f.nii\"]:\n","            # Get image path based on folder name and modality\n","            img_path = os.path.join(folder, folder.split(\"/\")[-1] + modality) \n","            # To handle Kaggle messing up zipped folders:\n","            try:\n","                img = nib.load(img_path).get_fdata()\n","            except nib.filebasedimages.ImageFileError: \n","                file = os.listdir(img_path)[0]\n","                img_path = os.path.join(img_path, file)\n","                img = nib.load(img_path).get_fdata()\n","            # Crop irrelevant parts of the volume\n","            if not self.test:\n","                img = img[35:-25,40:-20,:140] \n","            # Standardizing considering only the brain volume\n","            img[img > 0] = (img[img > 0] - np.mean(img[img > 0])) / np.std(img[img > 0]) \n","            img = torch.tensor(img, dtype=torch.float32)\n","            imgs.append(img)\n","        # Stack as a 4 channel input; 1 channel per modality\n","        combined_img = torch.stack(imgs) \n","        return combined_img\n","\n","\n","    def preprocess_masks(self, folder):\n","        mask_path = os.path.join(folder, folder.split(\"/\")[-1] + \"-seg.nii\")\n","        mask = nib.load(mask_path).get_fdata()\n","        mask = mask[35:-25,40:-20,:140]\n","        mask = torch.tensor(mask, dtype=torch.uint8)\n","        # Reassign mask labels considering the challenge targets:\n","        # Label 1 - ET: considers only ET\n","        mask_ET = torch.zeros_like(mask)\n","        mask_ET[mask == 3] = 1\n","        # Label 2 - TC: considers only NCR and Edema\n","        mask_TC = torch.zeros_like(mask)\n","        mask_TC[(mask == 1) | (mask == 3)] = 1\n","        # Label 3 - WT: considers every part of the tumor\n","        mask_WT = torch.zeros_like(mask)\n","        mask_WT[(mask == 1) | (mask == 2) | (mask == 3)] = 1\n","        # Stack as a 3 channel mask; 1 channel per label\n","        mask = torch.stack([mask_ET, mask_TC, mask_WT]) \n","        return mask\n","    \n","                                          \n","    def __len__(self):\n","        return len(self.folder_paths)"]},{"cell_type":"markdown","metadata":{},"source":["## Model loss function and evaluation metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TotalDiceLoss(nn.Module):\n","    def __init__(self, smooth=1.):\n","        super().__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, outputs, targets):\n","        outputs = F.sigmoid(outputs)\n","        total_loss = 0\n","        for channel in range(3):\n","            o_flat = outputs[:,channel,:,:,:].contiguous().view(-1)\n","            t_flat = targets[:,channel,:,:,:].contiguous().view(-1)\n","            intersection = torch.sum(o_flat * t_flat)\n","            sums = torch.sum(o_flat) + torch.sum(t_flat)\n","            dice = (2. * intersection + self.smooth) / (sums + self.smooth)\n","            total_loss += 1 - dice\n","        return total_loss / 3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def dice_per_class(outputs, targets, mean_only=False, d_slice=None, smooth=1.):\n","    outputs = F.sigmoid(outputs)\n","    outputs = torch.where(outputs > 0.5, torch.tensor(1), torch.tensor(0))\n","    dice_et = None\n","    dice_tc = None\n","    dice_wt = None\n","    for channel in range(3):\n","        if d_slice is not None:\n","            o_flat = outputs[channel,:,:,d_slice].contiguous().view(-1)\n","            t_flat = targets[channel,:,:,d_slice].contiguous().view(-1)\n","        else:\n","            o_flat = outputs[:,channel,:,:,:].contiguous().view(-1)\n","            t_flat = targets[:,channel,:,:,:].contiguous().view(-1)\n","        intersection = torch.sum(o_flat * t_flat)\n","        sums = torch.sum(o_flat) + torch.sum(t_flat)\n","        dice = (2. * intersection + smooth) / (sums + smooth)\n","        if channel == 0:\n","            dice_et = dice.item()\n","        elif channel == 1:\n","            dice_tc = dice.item()\n","        elif channel == 2:\n","            dice_wt = dice.item()\n","    if mean_only:\n","        return (dice_et + dice_tc + dice_wt) / 3\n","    return dice_et, dice_tc, dice_wt"]},{"cell_type":"markdown","metadata":{},"source":["## Dilated U-Net"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.727331Z","iopub.status.busy":"2024-05-29T01:32:41.727013Z","iopub.status.idle":"2024-05-29T01:32:41.749137Z","shell.execute_reply":"2024-05-29T01:32:41.748176Z","shell.execute_reply.started":"2024-05-29T01:32:41.727307Z"},"trusted":true},"outputs":[],"source":["class DoubleConvEncoder(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.double_conv(x)\n","        return x\n","\n","\n","class DoubleConvDecoder(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels*2, in_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(in_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.double_conv(x)\n","        return x\n","    \n","\n","class DoubleDilatedConv(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.double_dilated_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=2, dilation=2),\n","            nn.InstanceNorm3d(in_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=2, dilation=2),\n","            nn.InstanceNorm3d(in_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","    def forward(self, x):\n","        supervision1 = self.double_dilated_conv(x)\n","        out = torch.cat([supervision1, x], 1)\n","        return out, supervision1\n","        \n","        \n","class DilatedConvBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = DoubleConvEncoder(in_channels, out_channels)\n","        self.double_dilated_conv = DoubleDilatedConv(out_channels)\n","        self.conv_out = nn.Sequential(\n","            nn.Conv3d(out_channels*2, in_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(in_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.double_conv(x)\n","        x, supervision1 = self.double_dilated_conv(x)\n","        x = self.conv_out(x)\n","        return x, supervision1\n","\n","\n","class DownSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = DoubleConvEncoder(in_channels, out_channels)\n","        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        concat = self.double_conv(x)\n","        x = self.pool(concat)\n","        return x, concat\n","\n","\n","class UpSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.Upsample(scale_factor=2., mode=\"trilinear\", align_corners=True)\n","        self.double_conv = DoubleConvDecoder(in_channels, out_channels)\n","\n","    def forward(self, x, concat):\n","        x = self.up(x)\n","        if x.size() != concat.size():\n","            x = F.interpolate(x, size=concat.size()[2:], mode='trilinear', align_corners=True)\n","        x = torch.cat([x, concat], 1)\n","        x = self.double_conv(x)\n","        return x\n","    \n","\n","# Initialize weights with He initialization\n","def weights_init_he(m):\n","    if isinstance(m, nn.Conv3d):\n","        nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 1e-4)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.751406Z","iopub.status.busy":"2024-05-29T01:32:41.751123Z","iopub.status.idle":"2024-05-29T01:32:41.764754Z","shell.execute_reply":"2024-05-29T01:32:41.763860Z","shell.execute_reply.started":"2024-05-29T01:32:41.751385Z"},"trusted":true},"outputs":[],"source":["class DilatedUNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.down_conv1 = DownSample(in_channels, 48)\n","        self.down_conv2 = DownSample(48, 96)\n","        self.down_conv3 = DownSample(96, 192)\n","        self.bottle_neck = DilatedConvBottleneck(192, 384)\n","        self.up_conv1 = UpSample(192, 96)\n","        self.up_conv2 = UpSample(96, 48)\n","        self.up_conv3 = UpSample(48, 48)\n","        self.out1 = nn.Conv3d(384, num_classes, kernel_size=1)\n","        self.out2 = nn.Conv3d(192, num_classes, kernel_size=1)\n","        self.out3 = nn.Conv3d(96, num_classes, kernel_size=1)\n","        self.out4 = nn.Conv3d(48, num_classes, kernel_size=1)\n","        self.final_out = nn.Conv3d(48, num_classes, kernel_size=1)\n","        self.apply(weights_init_he)\n","\n","    def forward(self, x):\n","        x, down1 = self.down_conv1(x)\n","        x, down2 = self.down_conv2(x)\n","        x, down3 = self.down_conv3(x)\n","        supervision2, supervision1 = self.bottle_neck(x)\n","        supervision3 = self.up_conv1(supervision2, down3)\n","        supervision4 = self.up_conv2(supervision3, down2)\n","        x = self.up_conv3(supervision4, down1)\n","        \n","        final_out = self.final_out(x)\n","        supervision4 = F.interpolate(self.out4(supervision4), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        supervision3 = F.interpolate(self.out3(supervision3), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        supervision2 = F.interpolate(self.out2(supervision2), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        supervision1 = F.interpolate(self.out1(supervision1), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        return final_out, supervision4, supervision3, supervision2, supervision1"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline nnU-Net"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.766069Z","iopub.status.busy":"2024-05-29T01:32:41.765812Z","iopub.status.idle":"2024-05-29T01:32:41.778854Z","shell.execute_reply":"2024-05-29T01:32:41.778016Z","shell.execute_reply.started":"2024-05-29T01:32:41.766048Z"},"trusted":true},"outputs":[],"source":["class nnUNet_DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(out_channels),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.InstanceNorm3d(out_channels),\n","            nn.LeakyReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.double_conv(x)\n","        return x      \n","\n","\n","class nnUNet_DownSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nnUNet_DoubleConv(in_channels, out_channels)\n","        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        concat = self.double_conv(x)\n","        x = self.pool(concat)\n","        return x, concat\n","\n","\n","class nnUNet_UpSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up_conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n","        self.double_conv = nnUNet_DoubleConv(out_channels*2, out_channels)\n","\n","    def forward(self, x, concat):\n","        x = self.up_conv(x)\n","        if x.size() != concat.size():\n","            x = F.interpolate(x, size=concat.size()[2:], mode='trilinear', align_corners=True)\n","        x = torch.cat([x, concat], 1)\n","        x = self.double_conv(x)\n","        return x\n","    \n","\n","# Initialize weights with He initialization\n","def weights_init_he_nnunet(m):\n","    if isinstance(m, nn.Conv3d):\n","        nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 1e-4)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.780150Z","iopub.status.busy":"2024-05-29T01:32:41.779878Z","iopub.status.idle":"2024-05-29T01:32:41.794182Z","shell.execute_reply":"2024-05-29T01:32:41.793291Z","shell.execute_reply.started":"2024-05-29T01:32:41.780128Z"},"trusted":true},"outputs":[],"source":["class nnUNet(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.encoder1 = nnUNet_DownSample(in_channels, 32)\n","        self.encoder2 = nnUNet_DownSample(32, 64)\n","        self.encoder3 = nnUNet_DownSample(64, 128)\n","        self.encoder4 = nnUNet_DownSample(128, 256)\n","        self.encoder5 = nnUNet_DownSample(256, 320)\n","        self.bottle_neck = nnUNet_DoubleConv(320, 320)\n","        self.decoder1 = nnUNet_UpSample(320, 320)\n","        self.decoder2 = nnUNet_UpSample(320, 256)\n","        self.decoder3 = nnUNet_UpSample(256, 128)\n","        self.decoder4 = nnUNet_UpSample(128, 64)\n","        self.decoder5 = nnUNet_UpSample(64, 32)\n","        self.out1 = nn.Conv3d(256, num_classes, kernel_size=1)\n","        self.out2 = nn.Conv3d(128, num_classes, kernel_size=1)\n","        self.out3 = nn.Conv3d(64, num_classes, kernel_size=1)\n","        self.final_out = nn.Conv3d(32, num_classes, kernel_size=1)\n","        self.apply(weights_init_he_nnunet)\n","\n","    def forward(self, x):\n","        x, down1 = self.encoder1(x)\n","        x, down2 = self.encoder2(x)\n","        x, down3 = self.encoder3(x)\n","        x, down4 = self.encoder4(x)\n","        x, down5 = self.encoder5(x)\n","        x = self.bottle_neck(x)\n","        x = self.decoder1(x, down5)\n","        supervision1 = self.decoder2(x, down4)\n","        supervision2 = self.decoder3(supervision1, down3)\n","        supervision3 = self.decoder4(supervision2, down2)\n","        x = self.decoder5(supervision3, down1)\n","        \n","        final_out = self.final_out(x)\n","        supervision3 = F.interpolate(self.out3(supervision3), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        supervision2 = F.interpolate(self.out2(supervision2), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        supervision1 = F.interpolate(self.out1(supervision1), final_out.size()[2:], mode=\"trilinear\", align_corners=True)\n","        return final_out, supervision3, supervision2, supervision1"]},{"cell_type":"markdown","metadata":{},"source":["## Train and validation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_pred_vs_truth(preds, mask, img, dices, d_slice):    \n","    fig = plt.figure(figsize=(15,5))\n","    for i in range(len(preds)-1):\n","        preds[i] = torch.where(preds[i] > 0.5, torch.tensor(255), torch.tensor(0))\n","        ax = fig.add_subplot(2,len(preds)-1,i+1)\n","        plt.imshow(preds[i][:,:,:,d_slice].permute(1,2,0))\n","        ax.set_title(f\"Dice: {dices[i]:.4f} / Supervision level {i+1}\")\n","        \n","    preds[-1] = torch.where(preds[-1] > 0.5, torch.tensor(255), torch.tensor(0))\n","    ax = fig.add_subplot(2,len(preds)-1,len(preds))\n","    plt.imshow(preds[-1][:,:,:,d_slice].permute(1,2,0))\n","    ax.set_title(f\"Dice: {dices[-1]:.4f} / Final prediction\")\n","    \n","    ax = fig.add_subplot(2,len(preds)-1,len(preds)+1)\n","    plt.imshow(mask[:,:,:,d_slice].permute(1,2,0) * 255)\n","    ax.set_title(\"Ground truth\")\n","    \n","    ax = fig.add_subplot(2,len(preds)-1,len(preds)+2)\n","    plt.imshow(img[3,:,:,d_slice], cmap=\"gray\")\n","    ax.set_title(\"T2-FLAIR\")\n","    \n","    plt.tight_layout()\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_metrics(writer, running_loss, running_dice, epoch, loader_size, i, step_size, val=False):\n","    global CHECKPOINT\n","    if val:\n","        writer.add_scalar('Dice/Val/Mean', running_dice[0] / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","        writer.add_scalar('Dice/Val/ET', running_dice[1] / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","        writer.add_scalar('Dice/Val/TC', running_dice[2] / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","        writer.add_scalar('Dice/Val/WT', running_dice[3] / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","    else:\n","        writer.add_scalar('Loss/Train/', running_loss / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","        writer.add_scalar('Dice/Train/', running_dice / step_size, (epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","    writer.flush()\n","\n","\n","def save_plots(writer, preds, mask, img, epoch, loader_size, i, d_slice=85, val=False):\n","    global CHECKPOINT\n","    preds = [pred[0].cpu().detach() for pred in preds]\n","    mask = mask[0].cpu().detach()\n","    img = img[0].cpu().detach()\n","    dices = [dice_per_class(pred, mask, mean_only=True, d_slice=d_slice) for pred in preds]\n","    writer.add_figure('Pred-Truth/Val' if val else 'Pred-Truth/Train',\n","                      plot_pred_vs_truth(preds, mask, img, dices, d_slice=d_slice),\n","                      global_step=(epoch + CHECKPOINT - 1) * loader_size + i + 1)\n","    writer.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_checkpoint(epoch, prev_point, model, optimizer, scheduler):\n","    states = {\n","        \"epoch\": epoch + prev_point,\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"scheduler_state_dict\": scheduler.state_dict()\n","    }\n","    torch.save(states, f\"{MODEL_SAVE_PATH}/{MODEL_NAME}-epoch{epoch + prev_point}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}.pth\")\n","    \n","    \n","def load_checkpoint(model, optimizer, scheduler, load_path):\n","    states = torch.load(load_path)\n","    epoch = states[\"epoch\"]\n","    model.load_state_dict(states[\"model_state_dict\"])\n","    optimizer.load_state_dict(states[\"optimizer_state_dict\"])\n","    scheduler.load_state_dict(states[\"scheduler_state_dict\"])\n","    scheduler.step()\n","    return epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(writer, model, loader, loss_fn1, loss_fn2, optimizer, epoch):\n","    running_total_loss_per_batch = 0.0\n","    running_dice_per_batch = 0.0\n","    \n","    model.train()\n","    for i, (imgs, masks) in enumerate(loader):\n","        imgs = imgs.to(DEVICE)\n","        masks = masks.to(DEVICE)\n","        \n","        imgs_to_restack = []\n","        masks_to_restack = []\n","        for j in range(imgs.shape[0]):\n","            data_augmentation = Augmentations(imgs[j], masks[j])\n","            img, mask = data_augmentation.transforms()\n","            img = F.interpolate(img.unsqueeze(0), (128,128,128), mode=\"trilinear\", align_corners=True).squeeze(0)\n","            mask = F.interpolate(mask.unsqueeze(0), (128,128,128), mode=\"nearest\").squeeze(0)\n","            imgs_to_restack.append(img)\n","            masks_to_restack.append(mask)\n","        imgs = torch.stack(imgs_to_restack)\n","        masks = torch.stack(masks_to_restack)\n","        \n","        optimizer.zero_grad()\n","        \n","        preds = model(imgs)\n","        \n","        if loss_fn2 is not None:\n","            losses = [loss_fn1(pred, masks) + loss_fn2(pred, masks.to(torch.float32)) for pred in preds]\n","        else:\n","            losses = [loss_fn1(pred, masks) for pred in preds]\n","        total_loss = sum(losses)\n","        \n","        running_total_loss_per_batch += total_loss.item()\n","        running_dice_per_batch += 1 - loss_fn1(preds[0], masks).item()\n","        \n","        if i % 20 == 19:\n","            save_metrics(writer, running_total_loss_per_batch, running_dice_per_batch, epoch, len(loader), i, step_size=20)\n","            running_total_loss_per_batch = 0.0\n","            running_dice_per_batch = 0.0\n","        if i % 100 == 99:\n","            save_plots(writer, preds[::-1], masks, imgs, epoch, len(loader), i)\n","\n","        total_loss.backward()\n","        optimizer.step()\n","        \n","\n","def eval_model(writer, model, loader, epoch):\n","    running_mean_dice_per_batch = 0.0\n","    running_dice_et_per_batch = 0.0\n","    running_dice_tc_per_batch = 0.0\n","    running_dice_wt_per_batch = 0.0\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for i, (imgs, masks) in enumerate(loader):\n","            imgs = imgs.to(DEVICE)\n","            masks = masks.to(DEVICE)\n","            imgs = F.interpolate(imgs, (128,128,128), mode=\"trilinear\", align_corners=True)\n","            masks = F.interpolate(masks, (128,128,128), mode=\"nearest\")\n","            \n","            preds = model(imgs)\n","            \n","            final_dices = dice_per_class(preds[0], masks)\n","            final_mean_dice = sum(final_dices) / 3\n","            running_mean_dice_per_batch += final_mean_dice\n","            running_dice_et_per_batch += final_dices[0]\n","            running_dice_tc_per_batch += final_dices[1]\n","            running_dice_wt_per_batch += final_dices[2]\n","            \n","            if i % 5 == 4:\n","                running_dices = [running_mean_dice_per_batch, running_dice_et_per_batch, running_dice_tc_per_batch, running_dice_wt_per_batch]\n","                save_metrics(writer, None, running_dices, epoch, len(loader), i, step_size=5, val=True)\n","                running_mean_dice_per_batch = 0.0\n","                running_dice_et_per_batch = 0.0\n","                running_dice_tc_per_batch = 0.0\n","                running_dice_wt_per_batch = 0.0\n","            if i % 20 == 19:\n","                save_plots(writer, preds[::-1], masks, imgs, epoch, len(loader), i, val=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def trainer(CHECKPOINT, writer, model, train_dataloader, val_dataloader, loss_fn1, loss_fn2, optimizer, scheduler):\n","    for epoch in range(1,EPOCHS+1):\n","        train_model(writer, model, train_dataloader, loss_fn1, loss_fn2, optimizer, epoch)\n","        if epoch % 2 == 1:\n","            save_checkpoint(epoch, CHECKPOINT, model, optimizer, scheduler)\n","            eval_model(writer, model, val_dataloader, epoch)\n","        writer.add_scalar('LR', scheduler.get_last_lr()[0], epoch + CHECKPOINT)\n","        writer.flush()\n","        scheduler.step()"]},{"cell_type":"markdown","metadata":{},"source":["## Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MODEL_TO_TRAIN = \"nnunet\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"dilatedunet\":\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    SPLIT = [0.8, 0.2]\n","    LEARNING_RATE = 1e-2\n","    BATCH_SIZE = 2\n","    EPOCHS = 15\n","    TRAIN_PATHS = [\"/kaggle/input/brats2023-part1\", \"/kaggle/input/brats2023-part2\", \"/kaggle/input/brats2023-part3\"]\n","    MODEL_NAME = \"dilatedunet_default\"\n","    MODEL_SAVE_PATH = f\"/kaggle/working/{MODEL_NAME}/\"\n","    RESUME_CHECKPOINT_PATH = None\n","    os.makedirs(MODEL_SAVE_PATH)\n","\n","    train_dataset = BratsDataset(TRAIN_PATHS)\n","\n","    train_dataset, val_dataset = random_split(train_dataset, SPLIT, generator=torch.Generator().manual_seed(13))\n","\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    CHECKPOINT = 0\n","    model = DilatedUNet(in_channels=4, num_classes=3).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 15)\n","    loss_fn1 = TotalDiceLoss()\n","    loss_fn2 = nn.BCEWithLogitsLoss()\n","    if RESUME_CHECKPOINT_PATH is not None:\n","        CHECKPOINT = load_checkpoint(model, optimizer, scheduler, RESUME_CHECKPOINT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"dilatedunet\":\n","    writer_dilated = SummaryWriter(f'/kaggle/working/runs/{MODEL_NAME}_{CHECKPOINT}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"dilatedunet\":\n","    trainer(CHECKPOINT, writer_dilated, model, train_dataloader, val_dataloader, loss_fn1, loss_fn2, optimizer, scheduler)"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"nnunet\":\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    SPLIT = [0.8, 0.2]\n","    LEARNING_RATE = 1e-4\n","    BATCH_SIZE = 2\n","    EPOCHS = 15\n","    TRAIN_PATHS = [\"/kaggle/input/brats2023-part1\", \"/kaggle/input/brats2023-part2\", \"/kaggle/input/brats2023-part3\"]\n","    MODEL_NAME = \"nnunet_default\"\n","    MODEL_SAVE_PATH = f\"/kaggle/working/{MODEL_NAME}/\"\n","    RESUME_CHECKPOINT_PATH = None\n","    os.makedirs(MODEL_SAVE_PATH)\n","\n","    train_dataset = BratsDataset(TRAIN_PATHS)\n","\n","    train_dataset, val_dataset = random_split(train_dataset, SPLIT, generator=torch.Generator().manual_seed(13))\n","\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    model = nnUNet(in_channels=4, num_classes=3).to(DEVICE)\n","    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n","    scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=15, power=0.9)\n","    loss_fn1 = TotalDiceLoss()\n","    loss_fn2 = nn.BCEWithLogitsLoss()\n","    CHECKPOINT = 0\n","    if RESUME_CHECKPOINT_PATH is not None:\n","        CHECKPOINT = load_checkpoint(model, optimizer, scheduler, RESUME_CHECKPOINT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"nnunet\":\n","    writer_nnunet = SummaryWriter(f'/kaggle/working/runs/{MODEL_NAME}_{CHECKPOINT}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MODEL_TO_TRAIN == \"nnunet\":\n","    trainer(CHECKPOINT, writer_nnunet, model, train_dataloader, val_dataloader, loss_fn1, loss_fn2, optimizer, scheduler)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.808297Z","iopub.status.busy":"2024-05-29T01:32:41.808022Z","iopub.status.idle":"2024-05-29T01:32:41.817884Z","shell.execute_reply":"2024-05-29T01:32:41.816908Z","shell.execute_reply.started":"2024-05-29T01:32:41.808276Z"},"trusted":true},"outputs":[],"source":["def plot_test_pred(img, pred_mask, file_name, d_slice=80):\n","    img = F.interpolate(img, (240, 240, 155), mode=\"trilinear\", align_corners=True)\n","    img = img.squeeze(0).cpu().detach()\n","    \n","    fig = plt.figure(figsize=(15,5))\n","    plt.suptitle(f'Prediction for case <{file_name}> | z-slice no. {d_slice}', y=0.85)\n","    ax = fig.add_subplot(1,5,1)\n","    plt.imshow(img[1,:,:,d_slice], cmap=\"gray\")\n","    ax.set_title(\"T1-native\")\n","    ax = fig.add_subplot(1,5,2)\n","    plt.imshow(img[0,:,:,d_slice], cmap=\"gray\")\n","    ax.set_title(\"T1-weighted\")\n","    ax = fig.add_subplot(1,5,3)\n","    plt.imshow(img[2,:,:,d_slice], cmap=\"gray\")\n","    ax.set_title(\"T2-weighted\")\n","    ax = fig.add_subplot(1,5,4)\n","    plt.imshow(img[3,:,:,d_slice], cmap=\"gray\")\n","    ax.set_title(\"T2-FLAIR\")\n","    ax = fig.add_subplot(1,5,5)\n","    plt.imshow(pred_mask[:,:,d_slice])\n","    \n","    return fig"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:32:41.819857Z","iopub.status.busy":"2024-05-29T01:32:41.819127Z","iopub.status.idle":"2024-05-29T01:32:41.834420Z","shell.execute_reply":"2024-05-29T01:32:41.833599Z","shell.execute_reply.started":"2024-05-29T01:32:41.819827Z"},"trusted":true},"outputs":[],"source":["def inference(test_path, model_state, model_name, device, writer):\n","    if model_name == \"dilatedunet\":\n","        model = DilatedUNet(in_channels=4, num_classes=3).to(device)\n","    elif model_name == \"nnunet\":\n","        model = nnUNet(in_channels=4, num_classes=3).to(device)\n","    else:\n","        raise NameError(\"Model not defined\")\n","    model.load_state_dict(model_state)\n","    model.eval()\n","    test_dataset = BratsDataset(test_path, test=True)\n","  \n","    for i, (img, folder) in tqdm(enumerate(test_dataset), total=len(test_dataset)):\n","        img = img.to(device)\n","        img = F.interpolate(img.unsqueeze(0), (128,128,128), mode=\"trilinear\", align_corners=True)\n","\n","        pred_mask = model(img)\n","\n","        # Upscale to final required resolution for evaluation\n","        pred_mask = F.interpolate(pred_mask[0], (240, 240, 155), mode=\"nearest\").squeeze(0)\n","        pred_mask = torch.where(pred_mask > 0.5, torch.tensor(1), torch.tensor(0))\n","\n","        # Post-processing: If number of voxels with ET < 200, turn ET to NCR to avoid bad online evaluation scores for ET\n","        one_channel_pred = torch.zeros((240,240,155))\n","        one_channel_pred[(pred_mask[1] - pred_mask[0]) == 1] = 1\n","        one_channel_pred[(pred_mask[2] - pred_mask[1]) == 1] = 2\n","        if torch.sum(pred_mask[0]) < 200:\n","            one_channel_pred[pred_mask[0] == 1] = 1\n","        else:\n","            one_channel_pred[pred_mask[0] == 1] = 3\n","\n","        # Saves the prediction mask as a nii file for challenge evaluation\n","        affine = np.eye(4)\n","        affine[:3, 3] = np.array([0, 239, 0])\n","        nib_pred = nib.Nifti1Image(one_channel_pred.numpy(), affine=affine, dtype=np.float64)\n","        file_name = f\"{folder.split('/')[-1]}.nii.gz\"\n","        nib.save(nib_pred, f\"{SAVE_PATH}/{model_name}/{file_name}\")\n","\n","        # Store the images of the predictions\n","        writer.add_figure(f'Test-Images/{model_name}',\n","                            plot_test_pred(img, one_channel_pred, file_name.replace(\".nii.gz\", \"\")),\n","                            global_step=i)\n","        writer.flush()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T02:15:17.178793Z","iopub.status.busy":"2024-05-29T02:15:17.177877Z","iopub.status.idle":"2024-05-29T02:15:20.370062Z","shell.execute_reply":"2024-05-29T02:15:20.369219Z","shell.execute_reply.started":"2024-05-29T02:15:17.178761Z"},"trusted":true},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","TEST_PATH = [\"/kaggle/input/brats2023-validation\"]\n","MODEL_PTH = \"/kaggle/input/nnunet/pytorch/epoch20/1/nnunet-training_checkpoints-epoch20.pth\"\n","SAVE_PATH = \"/kaggle/working/test_preds\"\n","MODEL_TO_TEST = \"nnunet\"\n","os.makedirs(f\"{SAVE_PATH}/{MODEL_TO_TEST}\")\n","\n","writer_test = SummaryWriter(f'/kaggle/working/runs/{MODEL_TO_TEST}/')\n","states = torch.load(MODEL_PTH, map_location=DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T02:15:21.206104Z","iopub.status.busy":"2024-05-29T02:15:21.205370Z","iopub.status.idle":"2024-05-29T02:22:57.188369Z","shell.execute_reply":"2024-05-29T02:22:57.187564Z","shell.execute_reply.started":"2024-05-29T02:15:21.206073Z"},"trusted":true},"outputs":[],"source":["inference(TEST_PATH, states[\"model_state_dict\"], MODEL_TO_TEST, DEVICE, writer_test)\n","shutil.make_archive('nnunet-epoch20', 'zip', f'/kaggle/working/test_preds/{MODEL_TO_TEST}')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4875494,"sourceId":8223480,"sourceType":"datasetVersion"},{"datasetId":4875505,"sourceId":8223493,"sourceType":"datasetVersion"},{"datasetId":4875543,"sourceId":8223541,"sourceType":"datasetVersion"},{"datasetId":4875599,"sourceId":8223600,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":38409,"sourceId":45810,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":38410,"sourceId":45811,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":40665,"sourceId":48615,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":40666,"sourceId":48616,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":48013,"sourceId":57239,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":48014,"sourceId":57240,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
